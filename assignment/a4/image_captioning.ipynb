{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2c581a3"
      },
      "source": [
        "# Assignment 4: Image Captioning\n",
        "\n",
        "This assignment is somewhat short.  We want you to spend your time on the project instead!\n",
        "\n",
        "This assignment explores models connecting different modalities - exploring a connection between images and text.  By the time you're done with this assignment, you'll have:\n",
        "\n",
        "* investigated a few captioning techniques\n",
        "* worked with CLIP embeddings for images and captions\n",
        "* worked with the BLIP image captioning system\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-spring-main/blob/master/assignment/a4/image_captioning.ipynb)\n"
      ],
      "id": "b2c581a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "600d7481"
      },
      "source": [
        "# Foundational image captioning papers\n",
        "\n",
        "## Show & Tell\n",
        "\n",
        "[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf) was the first step towards neural image captioning.  Fundamentally it is an encoder-decoder scheme similar to what we've seen in class.  Concretely, it uses the CNN structure of an (at the time) state of the art image classification CNN as the encoder and it uses an LSTM as a decoder.  As in the generation models in class, it continues to generate text until a special \"stop\" token is emitted.  After **reading** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part A)\n",
        "\n",
        "1.  What parts of the CNN were fine-tuned during the image caption generation training process?\n",
        "2.  What was the biggest concern when deciding how to train the model?\n",
        "3.  How was the encoded image representation input into the decoder?\n",
        "4.  Given we are \"translating\" from an image to a caption (without a length constraint), which evaluation metric did the authors determine was reasonable for a top line metric?\n",
        "5.  What beam width is equivalent to one where you select the highest probability word in each decoding step?\n",
        "\n",
        "\n",
        "## Deep Visual Alignment\n",
        "\n",
        "[Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf) is a fun read for which we will ask no questions.  Its critical insights are around understanding an image as a composition of regions, and building upon that understanding to construct both a caption for the whole image, but labels for its consistuent parts.\n",
        "\n",
        "## Show, Attend & Tell\n",
        "\n",
        "[Show, Attend & Tell](https://arxiv.org/pdf/1502.03044.pdf) applies the same \"provide the decoder more context, as directly as possible\" trick we've seen over the course: adding attention.  After **skimming** the paper, answer the following questions:\n",
        "\n",
        "### Questions (Part B)\n",
        "\n",
        "1. What is the model paying attention to?\n",
        "2. What do the figures with highlight shading represent in Figures 2, 3 and 5?\n",
        "\n",
        "# Exploring an MS COCO captioner\n",
        "\n",
        "There are many examples of image captioners ML engineers have built on the MS COCO dataset you explored. [This one](https://replicate.com/rmokady/clip_prefix_caption) uses a (more) modern large language model as its decoder, GPT-2.  \n",
        "\n",
        "* **Explore** the samples and play with using beam search and not.  What do you notice?\n",
        "\n",
        "This is an example from the Show & Tell paper of a low-quality caption (see figure 5).  The GPT-2 model proposes \"the car that person drove to the hospital.\" vs. \"A yellow school bus parked in a parking lot\" from the original paper. ![Misclassified](https://github.com/datasci-w266/2025-spring-main/blob/master/assignment/a4/littlecar.png?raw=1)"
      ],
      "id": "600d7481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "958d9ca0"
      },
      "source": [
        "# CLIP Embeddings and Image Classification\n",
        "\n",
        "The [CLIP paper](https://arxiv.org/pdf/2103.00020.pdf)  describes a system that emits encodings that represent both images and text captions. The system learns to match a picture with its caption so the encoding for the image and the encoding for an associated caption should have a very high cosine similarity.  Systems like DALL-E use CLIP embeddings to generate images based on a text description by using the text encoding to get the image encoding and then processing the image encoding to generate the final image.  We're going to use CLIP in the opposite direction.  Namely we're going to use CLIP embeddings to classify images, that is to score a set of captions for an image based on the image's content.\n"
      ],
      "id": "958d9ca0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f84ea56"
      },
      "source": [
        "We can use the HuggingFace implementation of CLIP to experiment with this multimodal capability. Since we are not fine-tuning it we do not need access to a GPU."
      ],
      "id": "8f84ea56"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0b343ffd"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ],
      "id": "0b343ffd"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m2GGQX43yxle"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers --upgrade"
      ],
      "id": "m2GGQX43yxle"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q79s6H9xyxlg"
      },
      "outputs": [],
      "source": [
        "!pip install -q invisible_watermark accelerate safetensors"
      ],
      "id": "q79s6H9xyxlg"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "41a16f22"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, TFCLIPModel"
      ],
      "id": "41a16f22"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af441fd4",
        "outputId": "fb441d08-58d7-4ef4-de35-811feb2ec675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFCLIPModel.\n",
            "\n",
            "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "id": "af441fd4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efbe368f"
      },
      "source": [
        "Now let's begin our experiment.  We're going to select two images that contain both zebras and cars.  They may contain other things as well.  We're also going to generate a set of captions that we will score.  Specifically, we'll pass the output for the captions through a softmax to give us a probability distribution over the four captions."
      ],
      "id": "efbe368f"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d2c169c",
        "outputId": "4ff8455f-eb98-4545-9083-92e85c174966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://farm1.staticflickr.com/9/15631288_605abb3096_z.jpg\n",
            "                         a photo of cars - 0.0014\n",
            "                    a photo of a giraffe - 0.0475\n",
            "            a photo of zebras in a field - 0.0151\n",
            "         a photo of some zebras and cars - 0.9360\n",
            "\n",
            "http://farm4.staticflickr.com/3057/3033996041_11293469b7_z.jpg\n",
            "                         a photo of cars - 0.0000\n",
            "                    a photo of a giraffe - 0.0000\n",
            "            a photo of zebras in a field - 0.9660\n",
            "         a photo of some zebras and cars - 0.0339\n"
          ]
        }
      ],
      "source": [
        "# Example tags: animal = zebra, transport = car\n",
        "\n",
        "urls = [\"http://farm1.staticflickr.com/9/15631288_605abb3096_z.jpg\", #zebras foreground, cars background\n",
        "        \"http://farm4.staticflickr.com/3057/3033996041_11293469b7_z.jpg\"]  #zebra foreground, tiny car background\n",
        "captions = [\"a photo of cars\",\n",
        "            \"a photo of a giraffe\",\n",
        "            \"a photo of zebras in a field\",\n",
        "            \"a photo of some zebras and cars\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ],
      "id": "4d2c169c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839d1bc4"
      },
      "source": [
        "The CLIP embeddings allow us to associate captions with images.  Specifically, we can build a classifier that assigns probabilities to each of the captions.  We want the highest probability to go to the most descriptive caption out of the four captions for the given image.  Notice here that even though both images contain zebras, one of them features a line of clearly visible cars.  The other image only has one small car off in the distance.  Note that the first image with the cars scores high for the caption of ```a photo of some zebras and cars``` because the zebras and cars are very visible.  The second image scores highest for ```a photo of zebras in a field``` but the small car is less noticed but scores above a zero."
      ],
      "id": "839d1bc4"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f19bc8b1",
        "outputId": "912d30df-fc94-4d80-a5f6-a92015a6471e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://farm1.staticflickr.com/8/10896131_6a184b48cb_z.jpg\n",
            "                        a photo of a dog - 0.0001\n",
            "        a photo of some dogs in a basket - 0.0378\n",
            "                       a photo of a bike - 0.0007\n",
            "        a photo of some dogs with a bike - 0.9614\n",
            "\n",
            "http://farm4.staticflickr.com/3082/2797293301_dd26fd613f_z.jpg\n",
            "                        a photo of a dog - 0.0005\n",
            "        a photo of some dogs in a basket - 0.0000\n",
            "                       a photo of a bike - 0.9586\n",
            "        a photo of some dogs with a bike - 0.0408\n"
          ]
        }
      ],
      "source": [
        "# Example tags: two dogs in bike, human bike tiny dog\n",
        "\n",
        "urls = [\"http://farm1.staticflickr.com/8/10896131_6a184b48cb_z.jpg\",  #2 dogs in bike basket\n",
        "        \"http://farm4.staticflickr.com/3082/2797293301_dd26fd613f_z.jpg\"] #human and bike with tiny dog\n",
        "captions = [\"a photo of a dog\",\n",
        "            \"a photo of some dogs in a basket\",\n",
        "            \"a photo of a bike\",\n",
        "            \"a photo of some dogs with a bike\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ],
      "id": "f19bc8b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2359ba6"
      },
      "source": [
        "Again, these two images both contain bicycles and dogs.  The first image is two dogs in a basket on the front of a bike.  While the bike is visible, the two dogs are the focus of the image.  The second image features a person with their bike.  The bike happens to contain a small dog.  We would expect the embeddings to reflect the different emphases of the photos and indeed they do."
      ],
      "id": "f2359ba6"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13595ef8",
        "outputId": "6569e71f-e56c-41c0-add0-86b1d28ffbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://farm1.staticflickr.com/124/405495389_d4316b1224_z.jpg\n",
            "                        a photo of a dog - 0.9990\n",
            "                  a photo of a motorbike - 0.0002\n",
            "                      a photo of a plane - 0.0008\n",
            "                   a photo of some bikes - 0.0000\n",
            "\n",
            "http://farm8.staticflickr.com/7194/6991675037_3c298541c0_z.jpg\n",
            "                        a photo of a dog - 0.0013\n",
            "                  a photo of a motorbike - 0.8967\n",
            "                      a photo of a plane - 0.0000\n",
            "                   a photo of some bikes - 0.1020\n"
          ]
        }
      ],
      "source": [
        "# Example tags: animal = dog, transport = bike\n",
        "\n",
        "urls = [\"http://farm1.staticflickr.com/124/405495389_d4316b1224_z.jpg\",   #dog foreground and tiny bikes background\n",
        "        \"http://farm8.staticflickr.com/7194/6991675037_3c298541c0_z.jpg\"] #motorbike foreground, many bikes and tiny dog background\n",
        "captions = [\"a photo of a dog\",\n",
        "            \"a photo of a motorbike\",\n",
        "            \"a photo of a plane\",\n",
        "            \"a photo of some bikes\"]\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ],
      "id": "13595ef8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9315486e"
      },
      "source": [
        "For the third example, the first image includes a dog in the foreground and a number of small bikes in the distant background.  You can look at the annotations associated with the image to see where these objects are located. The second image includes a motorbike/motorcycle in the the foreground but a number of bikes and a tiny dog in the background.  Again we're hand crafting these captions to include the items in the image but we want the score for the caption to reflect what's in the foreground of the image.  "
      ],
      "id": "9315486e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79e1c0c8"
      },
      "source": [
        "Now it is your turn.  You will essentially replicate the examples above but you will do it with images **you** select.  First you need to select *two* images for processing. Go to [the COCO Explorer](https://cocodataset.org/#explore), click on two tag icons: an animal (see icon column of animals) and a mode of transportation (see icon column of ), and search. (You pick which; you might have to try a few combinations until you get multiple image results.)\n",
        "\n",
        "Find two different images that each contain your animal and your mode of transportation.  It's okay if they contain other things as well.  If you click on the URL icon above each image, you'll see a link to the annotated image and the original (unlabeled) image. Put the original image link in the code cell below *your image 1 url* and *your image 2 url*, then create four captions that mention only one of the objects each vs both objects together. You can see the captions we created for the three examples above.  The goal is to get probabilities above 0.85 for the caption that best describes the first image and the caption that best describes the second image.\n",
        "\n",
        "As in the examples above, you must find a pair of images with the same two objects tagged in them, but which get different results for which caption has the highest probability according to the CLIP model.\n",
        "\n",
        "Note which object tags you used, and give a brief explanation of what looks different about the two images that you think made them get different CLIP results for the most likely caption.  Enter that explanation in the cell below.  You **do not need to enter it in the answers sheet**.  Just leave it in the notebook that you submit."
      ],
      "id": "79e1c0c8"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4d4f1767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4c5e98-f351-4e4d-f045-1cc904fb054b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://farm4.staticflickr.com/3115/3159085023_43934a0714_z.jpg\n",
            "a cat resting on a motorbike which is next to a car. - 0.9900\n",
            " a cat sitting on the seat of a scooter. - 0.0096\n",
            "a very large orange cat lying on the roof of a vehicle. - 0.0002\n",
            "a large yellow cat laying on a green car. - 0.0002\n",
            "\n",
            "http://farm1.staticflickr.com/140/325364305_2fd75c5668_z.jpg\n",
            "a cat resting on a motorbike which is next to a car. - 0.0009\n",
            " a cat sitting on the seat of a scooter. - 0.0000\n",
            "a very large orange cat lying on the roof of a vehicle. - 0.9893\n",
            "a large yellow cat laying on a green car. - 0.0098\n"
          ]
        }
      ],
      "source": [
        "# Example tags: animal = cat, transportation = car\n",
        "\n",
        "### YOUR CODE HERE\n",
        "urls = [\"http://farm4.staticflickr.com/3115/3159085023_43934a0714_z.jpg\",   # Image 1 URL\n",
        "        \"http://farm1.staticflickr.com/140/325364305_2fd75c5668_z.jpg\"] # Image 2 URL\n",
        "captions = [\"a cat resting on a motorbike which is next to a car.\",\n",
        "            \"a cat sitting on the seat of a scooter.\",\n",
        "            \"a very large orange cat lying on the roof of a vehicle.\",\n",
        "            \"a large yellow cat laying on a green car.\"]\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))"
      ],
      "id": "4d4f1767"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88c074e"
      },
      "source": [
        "### Questions (Part C)\n",
        "\n",
        "1. What is the animal tag you selected?\n",
        "\n",
        "2. What is the transportation tag you selected?\n",
        "\n",
        "3. What is the probability associated with the most likely caption for image 1?\n",
        "\n",
        "4. What is the probability associated with the most likely caption for image 2?\n",
        "\n",
        "**(Answer 5 below but do NOT enter your sentences in the answers file)**\n",
        "\n",
        "5. Why do you think the differences between your two images are reflected in the 4 captions you produced.  "
      ],
      "id": "d88c074e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. cat\n",
        "\n",
        "2. car\n",
        "\n",
        "3. 0.9900\n",
        "\n",
        "4. 0.9893"
      ],
      "metadata": {
        "id": "tS4wow-JumZv"
      },
      "id": "tS4wow-JumZv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57494c14"
      },
      "source": [
        "Please answer in two to four sentences right here:\n",
        "\n",
        "*BEGIN Q 5 ANSWER HERE*\n",
        "- The differences between the two images are reflected in the captions and their scores because CLIP captures the visual differences and relationships between objects in the images.\n",
        "- The captions that best describe these differences are likely to score higher, leading to different captions being selected for each image.\n",
        "- The images have different backgrounds and contexts. One image showed the animal and transportation in a natural setting, while the other showed them in an urban environment.\n",
        "\n",
        "\n",
        "*END Q 5 ANSWER HERE*\n"
      ],
      "id": "57494c14"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og-Wr_UHwzcr"
      },
      "source": [
        "We used CLIP to evaluate the captions and to select the best caption given a choice from four.  Now let's use a model named [BLIP](https://huggingface.co/docs/transformers/en/model_doc/blip) to generate the caption for an image."
      ],
      "id": "Og-Wr_UHwzcr"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pAs8evZmPcWc"
      },
      "outputs": [],
      "source": [
        "!pip install -q invisible_watermark transformers accelerate safetensors"
      ],
      "id": "pAs8evZmPcWc"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZFLVe238MDOe"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoProcessor, TFBlipForConditionalGeneration"
      ],
      "id": "ZFLVe238MDOe"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ChMoBaBzMDOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2728944c-9e02-42ce-818b-ceb2fd12546e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBlipForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBlipForConditionalGeneration were initialized from the model checkpoint at Salesforce/blip-image-captioning-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBlipForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bl_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "bl_model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
      ],
      "id": "ChMoBaBzMDOe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lIkQGQVMDOf"
      },
      "source": [
        "Now let's begin our experiment.  We're going re-use the two images you used in the previous CLIP exercise. Your images contain both the animal and the type of transporation you selected.  They may contain other things as well.  We're also going to generate a caption for each one that we will score.  Specifically, we'll pass the output for the captions through a softmax to give us a probability distribution over the four captions."
      ],
      "id": "2lIkQGQVMDOf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Apht0MwdRGT"
      },
      "source": [
        "First, let's generate a caption for your first image, the one in C3.  Paste the image URL into the spot below."
      ],
      "id": "0Apht0MwdRGT"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4b2fvKEKNO4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90768e4e-f613-4ecb-a47c-27c5543950ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TFBlipTextModel.get_extended_attention_mask at 0x7b4624b7b880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TFBlipTextModel.get_extended_attention_mask at 0x7b4624b7b880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a picture of a cat sitting on a motor\n"
          ]
        }
      ],
      "source": [
        "#image one URL\n",
        "\n",
        "### YOUR CODE HERE\n",
        "url = \"http://farm4.staticflickr.com/3115/3159085023_43934a0714_z.jpg\"\n",
        "### END YOUR CODE\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "text = \"A picture of\"\n",
        "\n",
        "inputs = bl_processor(images=image, text=text, return_tensors=\"tf\")\n",
        "\n",
        "#outputs = bl_model(**inputs)\n",
        "outputs = bl_model.generate(**inputs, max_new_tokens=25)\n",
        "\n",
        "print(bl_processor.decode(outputs[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "id": "4b2fvKEKNO4B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvju_LGnd2OL"
      },
      "source": [
        "Next, let's generate a caption for your second image, the one in C4.  Paste the image URL into the spot below."
      ],
      "id": "dvju_LGnd2OL"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "usMrEeHgUAle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2727ed2f-d993-4bb2-e6e1-6abeb36329bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a picture of a cat on a car\n"
          ]
        }
      ],
      "source": [
        "#image two URL\n",
        "\n",
        "### YOUR CODE HERE\n",
        "url = \"http://farm1.staticflickr.com/140/325364305_2fd75c5668_z.jpg\"\n",
        "### END YOUR CODE\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "text = \"A picture of\"\n",
        "\n",
        "inputs = bl_processor(images=image, text=text, return_tensors=\"tf\")\n",
        "\n",
        "#outputs = bl_model(**inputs)\n",
        "outputs = bl_model.generate(**inputs, max_new_tokens=25)\n",
        "\n",
        "print(bl_processor.decode(outputs[0], skip_special_tokens=True))"
      ],
      "id": "usMrEeHgUAle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-qHFJBO09tT"
      },
      "source": [
        "Now lets see how the captions you just generated work as describing your images.  We're going to use CLIP to evaluate the captions you just generated.  Fill out the cell below by copying the URLs for the images you selected with the animal and the transportation. Now copy the BLIP caption for your first image and past it in to caption #1.  Copy the the BLIP caption for your second image and past it in to caption #3. Now take the highest scoring caption for image #1 from question 3c and paste that caption into slot 2.  Then take the highest scoring caption for image #2 from question 4c and paste that caption into slot 4. Now rerun CLIP and look at the scores.  "
      ],
      "id": "c-qHFJBO09tT"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_r21tYUNyQP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e85533-1986-4774-ee91-d0f8f3c9d38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://farm4.staticflickr.com/3115/3159085023_43934a0714_z.jpg\n",
            "   a picture of a cat sitting on a motor - 0.0475\n",
            "a cat resting on a motorbike which is next to a car. - 0.9019\n",
            "             a picture of a cat on a car - 0.0504\n",
            "a very large orange cat lying on the roof of a vehicle. - 0.0002\n",
            "\n",
            "http://farm1.staticflickr.com/140/325364305_2fd75c5668_z.jpg\n",
            "   a picture of a cat sitting on a motor - 0.0008\n",
            "a cat resting on a motorbike which is next to a car. - 0.0009\n",
            "             a picture of a cat on a car - 0.0114\n",
            "a very large orange cat lying on the roof of a vehicle. - 0.9869\n"
          ]
        }
      ],
      "source": [
        "# Example tags from section C: animal = cat, transportation = car\n",
        "\n",
        "### YOUR CODE HERE\n",
        "urls = [\"http://farm4.staticflickr.com/3115/3159085023_43934a0714_z.jpg\",   #\n",
        "        \"http://farm1.staticflickr.com/140/325364305_2fd75c5668_z.jpg\"] #\n",
        "captions = [\"a picture of a cat sitting on a motor\",\n",
        "            \"a cat resting on a motorbike which is next to a car.\",\n",
        "            \"a picture of a cat on a car\",\n",
        "            \"a very large orange cat lying on the roof of a vehicle.\"]\n",
        "### END YOUR CODE\n",
        "            #\n",
        "for url in urls:\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=captions, images=image, return_tensors=\"tf\", padding=True\n",
        "    )\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "    print()\n",
        "    print(url)\n",
        "    for i, caption in enumerate(captions):\n",
        "        print('%40s - %.4f' % (caption, probs[0, i]))\n"
      ],
      "id": "_r21tYUNyQP5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ7kNcPnHLQJ"
      },
      "source": [
        "### Questions (Part D)\n",
        "\n",
        "1. Does the BLIP caption win or do other captions win for image #1?\n",
        "\n",
        "2. Does the BLIP caption win or do other captions win for image #2?\n",
        "\n",
        "3. What is the probability associated with the most likely caption for image #1?\n",
        "\n",
        "4. What is the probability associated with the most likely caption for image# 2?\n",
        "\n",
        "**(Answer 5 below but do NOT enter your sentences in the answers file)**\n",
        "\n",
        "5. Why do you think the winning caption scored higher than the 3 others?  "
      ],
      "id": "VZ7kNcPnHLQJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. other captions win\n",
        "\n",
        "2. other captions win\n",
        "\n",
        "3. 0.9019\n",
        "\n",
        "4. 0.9869"
      ],
      "metadata": {
        "id": "K4CeSqXnwT1N"
      },
      "id": "K4CeSqXnwT1N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL7gQsoXfa2L"
      },
      "source": [
        "Please answer Q 5 in two to four sentences right here:\n",
        "\n",
        "BEGIN Q 5 ANSWER HERE\n",
        "- The winning caption likely scored higher due to its accurate and detailed description of the image's content, alignment with the image's context, and fluency in language.\n",
        "- These factors contribute to a stronger association between the caption and the image, leading to a higher score from CLIP.\n",
        "- The winning caption have shared similarities with captions or descriptions found in CLIP's training data, making it more likely to be recognized and scored favorably.\n",
        "\n",
        "END Q 5 ANSWER HERE"
      ],
      "id": "gL7gQsoXfa2L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12026669"
      },
      "source": [
        "## Yay, you're done with your 266 homework.  Now focus on your project!"
      ],
      "id": "12026669"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}